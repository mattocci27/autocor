[
["index.html", "Welcome An evolving book 0.1 Objective", " Welcome This book is a practical introduction for modeling autocorrelaition using R. An evolving book This book is not intended to be static. Starting in April 2019, we use this book to teach functional programming in the Stanford Data Challenge Lab (DCL) course. The DCL functions as a testing ground for educational materials, as our students give us routine feedback on what they read and do in the course. We use this feedback to constantly improve our materials, including this book. The source for the book is also available on GitHub where we welcome suggestions for improvements. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. d 0.1 Objective A quick review for likelihood methods Understanding what autocorrelation is Getting an idea for handling autocorrelation system.time(bookdown::render_book(&quot;index.Rmd&quot;)) "],
["like.html", "1 Likelihood methods 1.1 Likelihood 1.2 Linear models (LMs) 1.3 Generalized Linear models (GLMs) 1.4 Generalized Linear mixed models (GLMMs)", " 1 Likelihood methods library(tidyverse) 1.1 Likelihood \\[ y_i ~\\sim Pois(\\lambda) \\\\ \\] #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Probability of \\(y_i\\) observed when mean = \\(\\lambda\\) Likelihood L(\\(\\lambda\\)) = (probability of \\(y_1\\) = 2) \\(\\times\\) (probability of \\(y_2\\) = 5) \\(\\times \\cdots \\times\\) (probability of \\(y_{100}\\) = 3) \\[ L(\\lambda) = \\Pi Pois(y_i | \\lambda) \\] \\[ logL(\\lambda) = \\Sigma log(Pois(y_i | \\lambda)) \\] #&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. #&gt; Using compatibility `.name_repair`. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_warnings()` to see where this warning was generated. #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 9 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 2 row(s) containing missing values (geom_path). #&gt; Warning: Removed 18 rows containing missing values (geom_point). #&gt; Warning: Removed 18 rows containing missing values (geom_bar). 1.1.1 Analytical approaches Maximum likelihood estimation Generalized linear models (GLMs) 1.1.2 Simulation approaches Markov chain Monte Carlo (MCMC) Bayesian 1.2 Linear models (LMs) Models assume normal distributions in their error terms. \\[ y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma) \\\\ \\] or \\[ y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i \\\\ \\epsilon_i \\sim N(0, \\sigma) \\] 1.3 Generalized Linear models (GLMs) Models assume exponential family distributions in their error terms. \\[ log\\lambda_i = \\beta_0 + \\beta_1 x_i \\\\ y_i \\sim Pois(\\lambda_i) \\] 1.4 Generalized Linear mixed models (GLMMs) Models assume exponential family distributions in their error terms Error terms also have independent different means among groups. \\[ y_{ij} = \\beta_0 + \\beta_1 x_{ij} +\\epsilon_i + r_j\\\\ \\epsilon_i \\sim N(0, \\sigma) \\\\ r_j \\sim N(0, \\phi) \\] "],
["intro.html", "2 What is autocorrelation? 2.1 Simple correlation 2.2 One dimensional autocorrelation 2.3 Two dimensional autocorrelation", " 2 What is autocorrelation? spatial library(tidyverse) 2.1 Simple correlation A variable y is correlated with a variable x and its slope is \\(\\beta_1\\) In likelihood methods, this statement can be written as: \\[ y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma) \\\\ \\] or \\[ y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i \\\\ \\epsilon_i \\sim N(0, \\sigma) \\] We have three parameters \\(\\beta_0, \\beta_1, \\sigma\\) in this simple linear model. set.seed(1234) N &lt;- 50 beta0 &lt;- 2 beta1 &lt;- 0.8 sigma &lt;- 0.3 x &lt;- rnorm(N, 0, 1) y &lt;- rnorm(N, beta0 + beta1 * x, sigma) dat &lt;- tibble(x, y, n = seq(1, N)) ggplot(dat, aes(x = x , y = y)) + geom_point() dat %&gt;% gather(xy, val, 1:2) %&gt;% ggplot(., aes(x = n , y = val, col = xy)) + geom_point() + geom_line() cor.test(x, y) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: x and y #&gt; t = 15, df = 48, p-value &lt;2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.849 0.949 #&gt; sample estimates: #&gt; cor #&gt; 0.912 2.2 One dimensional autocorrelation Previous observation(s) affects the current observation. Temporal autocorrelation is this type of the autocorrelation. moge \\[ x_i = \\mu_x +\\epsilon_{xi} \\\\ y_i = \\mu_y +\\epsilon_{yi} \\\\ \\epsilon_{y,i+1} \\sim N(\\epsilon_{y,i}, \\phi_y) \\\\ \\epsilon_{x,i+1} \\sim N(\\epsilon_{x,i}, \\phi_x) \\] N &lt;- 50 x &lt;- y &lt;- NULL x[1] &lt;- 0 y[1] &lt;- 0 for (i in 1:(N-1)) { y[i + 1] &lt;- rnorm(1, y[i], 0.8) x[i + 1] &lt;- rnorm(1, x[i], 0.8) } dat &lt;- tibble(x, y, n = seq(1, N)) ggplot(dat, aes(x = x , y = y)) + geom_point() dat %&gt;% gather(xy, val, 1:2) %&gt;% ggplot(., aes(x = n , y = val, col = xy)) + geom_point() + geom_line() cor.test(x, y) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: x and y #&gt; t = 2, df = 48, p-value = 0.05 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.00538 0.52060 #&gt; sample estimates: #&gt; cor #&gt; 0.283 We got a strong correlation for the two variables that are supposed to be independent. N &lt;- 50 r &lt;- NULL for (j in 1:200) { x &lt;- y &lt;- NULL x[1] &lt;- 0 y[1] &lt;- 0 for (i in 1:(N-1)) { y[i + 1] &lt;- rnorm(1, y[i], 0.8) x[i + 1] &lt;- rnorm(1, x[i], 0.8) } r[j] &lt;- cor(x, y) } qplot(r) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Each point shows daily measurements Do you believe this? Do you believe this? 2.3 Two dimensional autocorrelation Neighbors affect the current observation. Spatial autocorrelation is this type of the autocorrelation. 2D \\[ Y = X \\beta + \\rho W (Y - X \\beta) + \\epsilon \\] library(spdep) #&gt; Loading required package: sp #&gt; Loading required package: spData #&gt; Warning: multiple methods tables found for &#39;wkt&#39; #&gt; To access larger datasets in this package, install the spDataLarge #&gt; package with: `install.packages(&#39;spDataLarge&#39;, #&gt; repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` #&gt; Loading required package: sf #&gt; Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.2 library(Matrix) #&gt; #&gt; Attaching package: &#39;Matrix&#39; #&gt; The following objects are masked from &#39;package:tidyr&#39;: #&gt; #&gt; expand, pack, unpack set.seed(1234) N &lt;- 50 x &lt;- seq(1, 25) y &lt;- seq(1, 25) nb &lt;- cell2nb(length(x),length(y), type = &quot;queen&quot;) W &lt;- nb2listw(nb, zero.policy = TRUE, style = &quot;W&quot;) B &lt;- nb2mat(nb, zero.policy = TRUE, style = &quot;W&quot;) #z &lt;- matrix(numeric(length(x) * length(y)), ncol = length(y)) z &lt;- matrix(rnorm(length(x) * length(y)), ncol = length(y)) automean2&lt;-function(metric){ #queen?equal weighting 1/8 d.W&lt;-nb2listw(d.nb, zero.policy=TRUE,style=&quot;W&quot;) #SAR a&lt;-spautolm(metric~ 1,listw=d.W,na.action=na.omit) return(a$fit$fitted.values) } z0 &lt;- z z2 &lt;- z for (i in 2:(length(x)-1)) { for (j in 2:(length(y)-1)) { z[i, j] &lt;- rnorm(1, mean( c(z[i - 1, j - 1], z[i - 1, j], z[i - 1, j + 1], z[i, j - 1], z[i, j + 1], z[i + 1, j - 1], z[i + 1, j], z[i + 1, j + 1])), 0) } } z_scaled &lt;- (z - mean(as.numeric(z))) / sd(as.numeric(z)) z2 &lt;- z z2[z &gt; 0] &lt;- &quot;ridge&quot; z2[z &lt; 0] &lt;- &quot;valley&quot; #z2[z &gt; -0.5 &amp; z &lt; 0.5] &lt;- &quot;flat&quot; #z2 &lt;- z2 + z x2 &lt;- rep(x, length(y)) y2 &lt;- rep(y, each = length(x)) dat &lt;- tibble(x = x2, y = y2, mu = as.vector(z), hab = as.vector(z2), z = as.numeric(z_scaled), z0 = as.numeric(z0) ) ggplot(dat, aes(x = x, y = y, fill = z0)) + geom_raster() dat %&gt;% # cut the edges filter(x &gt; 1) %&gt;% filter(x &lt; max(x)) %&gt;% filter(y &gt; 1) %&gt;% filter(y &lt; max(y)) %&gt;% ggplot(., aes(x = x, y = y, fill = z)) + geom_raster() ggplot(dat, aes(x = x, y = y, fill = hab)) + geom_raster() ggplot(dat, aes(x = x, y = y, z = mu)) + geom_contour() tmp &lt;- rmvnorm(100, c(0, 0), diag(2)) %&gt;% rbind(., rmvnorm(100, c(2, 4), diag(2))) %&gt;% as_tibble moge &lt;- expand.grid(seq(3,-3, length = 100), seq(3,-3, length = 100)) moge %&gt;% mutate(p = dmvnorm(moge, c(0, 0), diag(2))) %&gt;% ggplot(., aes(x = Var1, y = Var2, z = p)) + geom_contour() m &lt;- ggplot(tmp, aes(x = V1, y = V2)) + geom_point() # contour lines m + geom_density_2d() dat2 &lt;- dat %&gt;% # mutate(hab_dummy = ifelse(hab == &quot;valley&quot;, 0, 1)) %&gt;% mutate(trait = rnorm(nrow(.), mu, 0.3)) # based on z dat2 %&gt;% ggplot(., aes(x = hab, y = trait, col = hab)) + geom_violin() + geom_jitter(width = 0.2) Ridge sites have greater trait values (e.g., thick dense leaves to grow well in dry conditions) Is this because those traits are favored in ridge sites? Is this just because neighbors have similar trait values (spatial autocorrelation)? "]
]
