
```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Likelihood methods

- Estimating parameters that most likely to produce the observed data (in a given probability distribution).
```{r message = FALSE, warning = FALSE, echo=FALSE}
library(tidyverse)
library(animation)
```

## Likelihood 

- Let's consier a simple example where the obsrervaions (y) follow the Poisson distribution.

$$
y_i ~\sim Pois(\lambda) \\
$$


```{r, echo=FALSE}
set.seed(123)
N <- 100 
lambda <- 3.5
y <- rpois(N, lambda)
qplot(y)

xx <- seq(0, 10)
p <- dpois(xx, lambda)

tibble(y = y, ID = 1:N) %>%
  DT::datatable(.)
```

```{r, echo=FALSE, eval=FALSE}
tibble(x = xx, y = p) %>%
  ggplot(., aes(x = x, y = y)) +
  geom_point() + 
  geom_line(lty = 2)
```

- Probability of $y_i$ observed when mean = $\lambda$ 
- Likelihood L($\lambda$) = (probability of $y_1$ = `r y[1]` when mean = $\lambda$ ) $\times$ (probability of $y_2$ = `r y[2]` when mean = $\lambda$ ) $\times \cdots \times$ (probability of $y_{100}$ = `r y[100]` when mean = $\lambda$ ) 


$$
L(\lambda) = \Pi Pois(y_i | \lambda)
$$

$$
logL(\lambda) = \Sigma log(Pois(y_i | \lambda))
$$


```{r, echo=FALSE, fig.height=10}

lambda_ <- seq(1.5, by = 0.4, length  = 9)
dat <- sapply(lambda_, function(x) dpois(xx, x)) %>%
  as_tibble

names(dat) <- str_c("lambda = ", lambda_) 
#names(dat) <- mean_

like1 <- tibble(lambda = str_c("lambda = ", lambda_)) %>%
  mutate(lambda = factor(lambda, 
                       levels = str_c("lambda = ", lambda_) )) %>%
  mutate(logL = sapply(lambda_, 
                       function(x) dpois(y, x, log = TRUE)) %>%
                       apply(., 2, sum) %>%
                       round(0))

dat_hist <- tibble(logL = like1$logL %>% rep(., each = N)) %>%
  mutate(y = rep(y, 9)) %>%
  mutate(lambda = like1$lambda %>% rep(., each = N))


dat %>%
  mutate(xx = xx) %>%
  pivot_longer(1:length(lambda_), names_to = "lambda", values_to = "p") %>%
  mutate(lambda = factor(lambda, 
                       levels = str_c("lambda = ", lambda_) )) %>%
  ggplot(.) +
  geom_line(aes(x = xx, y = p * 100), lty = 2) +
  geom_point(aes(x = xx, y = p * 100)) +
  geom_histogram(data = dat_hist, aes(x = y)) +
  geom_text(data = like1, aes(x = 5, y = 30, label = str_c("logL = ", logL))) +
  facet_wrap( ~ lambda, ncol = 3) +
  xlim(0, 8) +
  xlab("") +
  ylab("")

```

### Analytical approaches

- Maximum likelihood estimation
  - Generalized linear models (GLMs)

```{r, echo=FALSE}
lambda_seq <- seq(0.1, 10, length = 100)
logL <- sapply(lambda_seq, function(x)dpois(y, x, log = TRUE)) %>%
  apply(., 2, sum)


tibble(lambda = lambda_seq, logL) %>%
  ggplot(., aes(x = lambda, y = logL)) +
  geom_line(col = "blue") +
  geom_segment(aes(x = mean(y), xend = mean(y), 
                   y = min(logL),
                   yend = dpois(y, mean(y), log = TRUE) %>% sum),
              arrow = arrow(length = unit(0.03, "npc"))
  ) +
  geom_text(aes(x = 5, y = -600), 
            label = str_c("lambda = ", mean(y)))
```

```{r, echo=TRUE}
fit <- glm(y ~ 1, family = "poisson") 
fit[[1]] %>% exp
```


### Simulation approaches

- Markov chain Monte Carlo (MCMC)
  - Bayesian

```{r,eval=FALSE, echo=FALSE}

proposalfunction <- function(param){
    rpois(1, param)
}

posterior <- function(param){
    dpois(y, param, log = TRUE) %>% sum
}

run_metropolis_MCMC <- function(startvalue, iterations){
   chain <- NULL
   chain[1] <- startvalue
    for (i in 1:iterations){
        proposal = proposalfunction(chain[i])
        
        probab = exp(posterior(proposal) - posterior(chain[i]))
        if (runif(1) < probab){
            chain[i+1] = proposal
        }else{
            chain[i+1] = chain[i]
        }
    }
    return(chain)
}

iter <- 100
chain <- run_metropolis_MCMC(100, iter)

chain_dat <- tibble(chain = chain[-1], n = 1:iter)

pois_frame <- function(n, data) {
  pt_dat <- data %>%
   head(n)
  plt <- ggplot(pt_dat, aes(x = n, y = chain)) +
  geom_point() +
  geom_line() +
  xlab("Iteration") +
  ylab("lambda") +
  coord_cartesian(xlim = c(1, 80), ylim = c(0, 100)) 
  print(plt)
}

saveGIF(
       lapply(1:80, function(x)pois_frame(x, chain_dat)),
       interval = 0.05,
       ani.width = 480,
       ani.height = 240,
       movie.name = "pois.gif"
        )

```

![](pois.gif)


## Linear models (LMs)

- Models assume normal distributions in their error terms.

$$
y_i \sim N(\beta_0 + \beta_1 x_i, \sigma) \\
$$

- or

$$
y_i = \beta_0 + \beta_1 x_i +\epsilon_i \\
\epsilon_i \sim N(0, \sigma)
$$

```
lm(y ~ x, data = ... )
```

## Generalized Linear models (GLMs)

- Models assume exponential family distributions (e.g., Poisson, Gamma,
  Binomial, Negative-binomial,...) in their error terms.

$$
log\lambda_i = \beta_0 + \beta_1 x_i \\
y_i \sim Pois(\lambda_i)
$$


```
# poisson distribution and log link function
glm(y ~ x, data = ..., family = "poisson")
```

## Generalized Linear mixed models (GLMMs)

### Independent errors

- Models assume exponential family distributions in their error terms 
- Error terms also have different means among groups.

$$
y_{ij} = \beta_0 + \beta_1 x_{ij} +\epsilon_i + r_j\\
\epsilon_i \sim N(0, \sigma) \\
r_j \sim N(0, \phi)
$$


```
lme4::lmer(y ~ x + (x | group), data = ...)
```

### Dependent errors

- Error terms are sometimes dependent (correlated) among groups.
- We will learn this today (if we have a time).
